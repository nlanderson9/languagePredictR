#' @title Wrapped by node_edge function
#'
#' @param text Input text
#' @param maxDist Max distance between words
#' @param removeStopwords Whether to remove stopwords
#' @param showProgress Show a progress bar
#' @param level The level from the model (0 or 1)
#'
#' @noRd
#'
#' @importFrom rlang .data

make_node_edge_table = function(text, maxDist=4, removeStopwords=FALSE, showProgress=TRUE, level=NULL) {

  dist=indiv_count=inverse_mean_dist=cooc_count=first_count=second_count=NULL

  test = text %>% corpus() %>% tokens() %>% tokens_ngrams(n=1:(maxDist+1)) %>% as.list %>% unlist() %>% as.list()
  test = lapply(test, function(x) unlist(strsplit(x,"_")))

  get_words = function(x) {
    first = x[1]
    last = x[length(x)]
    alpha_first = ifelse(first < last, first, last)
    alpha_last = ifelse(first < last, last, first)
    dist = length(x)-1
    temp_frame = data.frame(first=alpha_first, second=alpha_last, dist=dist)
    return(temp_frame)
  }

  if(showProgress) {
    if (!is.null(level)) {
      print(paste("Creating table for '",level,"' text...", sep=""))
    }
    test_result = pblapply(test, get_words)
  }
  else {
    test_result = lapply(test, get_words)
  }
  test_frame = bind_rows(test_result)

  test_frame_summary = test_frame %>%
    subset(first != second) %>%
    group_by(first, second) %>%
    summarise(total_dist = sum(dist), inverse_mean_dist = 1/mean(dist), cooc_count=n(), .groups = "drop_last")

  test_frame_indiv = data.frame(word = c(test_frame$first, test_frame$second)) %>%
    group_by(word) %>%
    summarise(indiv_count = n(), .groups = "drop_last")

  test_frame_summary_complete = test_frame_summary %>%
    left_join(test_frame_indiv, by = c("first" = "word")) %>%
    rename(first_count = indiv_count) %>%
    left_join(test_frame_indiv, by = c("second" = "word")) %>%
    rename(second_count = indiv_count) %>%
    mutate(weight = inverse_mean_dist * ( (2*cooc_count) / (first_count + second_count) ))

  if(removeStopwords) {
    test_frame_summary_complete = subset(test_frame_summary_complete, !(first %in% stopwords()) & !(second %in% stopwords()))
  }

  return(test_frame_summary_complete)
}

#' @title Create Node-Edge Table
#' @description Creates a table of nodes and edges based on a language corpus. Edge weights represent the average distance between two words, corrected by their frequency of appearance.
#'
#' @param input Either a vector of strings, or a model generated by the \code{\link{language_model}} function
#' @param maxDist The maximum distance to consider two words being co-occurent. Default is 4 (i.e. for "I went to the store," "I" and "store" are 4 words apart)
#' @param removeStopwords If TRUE, words in \code{quanteda} \code{stopwords} function are excluded from analysis. Defaults to FALSE.
#' @param showProgress IF TRUE, progress bars are displayed. Defaults to TRUE.
#'
#' @return A dataframe with node and edge weight information, along with occurence counts. See "Details."
#'
#'@importFrom rlang .data
#'
#' @export
#'
#' @details
#' This function quantifies the relationship between words in the provided text. \cr
#' It computes a measure of inverse average distance between word pairs, and then multiples that by a Dice coefficient (to control for frequency of occurrence and co-occurence of words) \cr
#' Specifically, the formula used is:
#' \deqn{weight = \frac{1}{\bar{D}}*\frac{2*|X \cap Y|}{|X| + |Y|}}
#' where: \cr
#' \eqn{\bar{D}} = mean distance between word X and word Y \cr
#' \eqn{|X \cap Y|} = number of co-occurences of word X and word Y \cr
#' \eqn{|X|} = number of occurences of word X across all pairs \cr
#' \eqn{|Y|} = number of occurences of word Y across all pairs \cr
#' \cr
#' The output dataframe will contain the following columns: \cr
#' -**first** and **second** columns: the nodes specifying the two words of the pair \cr
#' -**inverse_mean_distance**: the mean distance between the word pair, computed as an inverse to give greater weight to words that are closer together (\eqn{\frac{1}{\bar{D}}}) \cr
#' -**cooc_count**: the number of co-occurences of the **first** and **second** words (\eqn{|X \cap Y|}) \cr
#' -**first_count**: the number of times the **first** word appears in a pair (\eqn{|X|}) \cr
#' -**second_count**: the number of times the **first** word appears in a pair (\eqn{|Y|}) \cr
#' -**weight**: the final weight, calculated as above
#'
#' @examples
#' \dontrun{
#' movie_review_data1$cleanText = clean_text(movie_review_data1$text)
#'
#' # Using language to predict "Positive" vs. "Negative" reviews
#' movie_model_valence = language_model(movie_review_data1,
#'                                      outcomeVariableColumnName = "valence",
#'                                      outcomeVariableType = "binary",
#'                                      textColumnName = "cleanText")
#'
#' node_edge_table = node_edge(movie_model_valence)
#'}
#'
node_edge = function(input, maxDist=4, removeStopwords=FALSE, showProgress = TRUE) {

  dist=indiv_count=inverse_mean_dist=cooc_count=first_count=second_count=total_dist=NULL

  if (class(input) == "langModel") {
    text = input@data_text
    outcome = input@data_outcome
    text_dataframe = data.frame(text=text, outcome=outcome)
    level0_text = subset(text_dataframe, outcome==input@level0)
    level1_text = subset(text_dataframe, outcome==input@level1)
    level0_result = make_node_edge_table(level0_text, maxDist = maxDist, removeStopwords = removeStopwords, level=input@level0, showProgress = showProgress)
    level1_result = make_node_edge_table(level1_text, maxDist = maxDist, removeStopwords = removeStopwords, level=input@level1, showProgress = showProgress)
    level0_result$outcome = input@level0
    level1_result$outcome = input@level1
    combined_result = bind_rows(level0_result, level1_result)
    overall_result = combined_result %>%
      group_by(first, second) %>%
      summarise(total_dist = sum(total_dist), cooc_count = sum(cooc_count), first_count = sum(first_count), second_count = sum(second_count), .groups = "drop_last") %>%
      mutate(inverse_mean_dist = 1 / (total_dist / cooc_count)) %>%
      mutate(weight = inverse_mean_dist * ( (2*cooc_count) / (first_count + second_count) ))
    overall_result$outcome = "all_outcomes"
    result = bind_rows(combined_result, overall_result)
  }
  else if (is.character(input)) {
    result = make_node_edge_table(input, maxDist = maxDist, removeStopwords = removeStopwords, level=NULL, showProgress = showProgress)
  }
  result = subset(result, select = -c(total_dist))
  return(result)
}
