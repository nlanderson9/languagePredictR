% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/language_model.R
\name{language_model}
\alias{language_model}
\title{Create Language Model}
\usage{
language_model(
  inputDataframe,
  outcomeVariableColumnName,
  outcomeVariableType,
  textColumnName,
  ngrams = "1",
  dfmWeightScheme = "count",
  lossMeasure = "deviance",
  lambda = "lambda.min",
  parallelCores = NULL,
  permutePValue = FALSE,
  permutationK = 1000,
  permuteByGroup = NULL,
  progressBar = TRUE
)
}
\arguments{
\item{inputDataframe}{A dataframe containing a column with text data (character strings) and an outcome variable (numeric or two-level factor)}

\item{outcomeVariableColumnName}{A string consisting of the column name for the outcome variable in \code{inputDataframe}}

\item{outcomeVariableType}{A string consisting of the type of outcome variable being used - options are "binary" or "continuous"}

\item{textColumnName}{A string consisting of the column name for the text data in \code{inputDataframe}}

\item{ngrams}{A string defining the ngrams to serve as predictors in the model. Defaults to "1". For more information, see the \code{okens_ngrams} function in the \code{quanteda} package}

\item{dfmWeightScheme}{A string defining the weight scheme you wish to use for constructing a document-frequency matrix. Default is "count". For more information, see the \code{dfm_weight} function in the \code{quanteda} package}

\item{lossMeasure}{A string defining the loss measure to use. Must be one of the options given by \code{cv.glmnet}. Default is "deviance".}

\item{lambda}{A string defining the lambda value to be used. Default is "lambda.min". For more information, see the \code{cv.glmnet} function in the \code{glmnet} package}

\item{parallelCores}{An integer defining the number of cores to use in parallel processing for model creation. Defaults to NULL (no parallel processing).}

\item{permutePValue}{If TRUE, a permutation test is run to estimate a p-value for the model (i.e. whether the language provided significantly predicts the outcome variable). Warning: this can take a while depending on the size of the dataset and number of permutations!}

\item{permutationK}{The number of permutations to run in a permutation test. Only used if \code{permutePValue = TRUE}. Defaults to 1000.}

\item{permuteByGroup}{A string consisting of the column name defining a grouping variable in the dataset (usually a participant number). This means that when permutations are randomized, they will permute items on a group level rather than trial level. Default is NULL (no group variable considered).}

\item{progressBar}{Show a progress bar. Defaults to TRUE.}
}
\value{
An object of the type "langModel"
}
\description{
This function creates a regression model using input text as predictors, and a specified variable as the outcome.
}
\details{
This is the core function of the \code{languagePredictR} package. It largely follows the analysis laid out in Dobbins & Kantner 2019 (see References).\cr\cr
In the broadest terms, this serves as a wrapper for the quanteda (text analysis) and glmnet (modeling) packages.\cr
The input text is converted into a document-frequency matrix (sometimes called a document-feature matrix) where each row represents a string of text, and each column represents a word that appears in the entire text corpus.\cr
Each cell is populated by a value defined by the dfmWeightScheme. For example, the default, "count", means that each word column contains a value representing the number of times that word appears in the given text string.\cr
This matrix is then used to train a regression algorithm appropriate to the outcome variable (standard linear regression for continuous variables, logistic regression for binary variables).\cr
See the documentation for the \code{\link[glmnet]{cv.glmnet}} function in the \code{glmnet} package for more information.\cr
10-fold cross validation is currently implemented to reduce overfitting to the data.\cr
Additionally, a LASSO constraint is used (following Tibshirani, 1996; see References) to eliminate weakly-predictive variables. This reduces the number of predictors (i.e. word engrams) to sparse, interpretable set.
}
\examples{

\dontrun{
movie_review_data1$cleanText = clean_text(movie_review_data1$text)

# Using language to predict "Positive" vs. "Negative" reviews
movie_model_valence = language_model(movie_review_data1,
                                     outcomeVariableColumnName = "valence",
                                     outcomeVariableType = "binary",
                                     textColumnName = "cleanText")

# Using language to predict 1-10 scale ratings,
# but using both unigrams and bigrams, as well as a proportion weighting scheme
movie_model_rating = language_model(movie_review_data1,
                                    outcomeVariableColumnName = "rating",
                                    outcomeVariableType = "continuous",
                                    textColumnName = "cleanText",
                                    ngrams = "1:2",
                                    dfmWeightScheme = "prop")
}

}
\references{
Dobbins, I. G., & Kantner, J. (2019). The language of accurate recognition memory. *Cognition, 192*, 103988.\cr
Tibshirani, R. (1996). Regression Shrinkage and Selection Via the Lasso. *Journal of the Royal Statistical Society: Series B (Methodological), 58*(1), 267-288.
}
